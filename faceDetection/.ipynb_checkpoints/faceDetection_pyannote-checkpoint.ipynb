{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a new `conda` environment:\n",
    "\n",
    "```bash\n",
    "$ conda create -n pyannote python=3.6 anaconda\n",
    "$ source activate pyannote\n",
    "```\n",
    "\n",
    "Then, install `pyannote-video` and its dependencies:\n",
    "\n",
    "```bash\n",
    "$ pip install pyannote-video\n",
    "```\n",
    "\n",
    "Finally, download sample video and `dlib` models:\n",
    "\n",
    "```bash\n",
    "$ git clone https://github.com/pyannote/pyannote-data.git\n",
    "$ git clone https://github.com/davisking/dlib-models.git\n",
    "$ bunzip2 dlib-models/dlib_face_recognition_resnet_model_v1.dat.bz2\n",
    "$ bunzip2 dlib-models/shape_predictor_68_face_landmarks.dat.bz2\n",
    "```\n",
    "\n",
    "To execute this notebook locally:\n",
    "```bash\n",
    "$ git clone https://github.com/pyannote/pyannote-video.git\n",
    "$ jupyter notebook --notebook-dir=\"pyannote-video/doc\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Les_varietes_resistantes___evaluation_sur_le_terrain',\n",
       " 'VITIS_PROHIBITA___EXTRAIT_13',\n",
       " 'VITIS_PROHIBITA___EXTRAIT_15',\n",
       " \"Des_vignes_resistantes_au_mildiou_avec_l'INRA_de_Gruissan\",\n",
       " 'Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud',\n",
       " '17-_Colloque_DEPHY_Viticulture_-_Cepages_resistants',\n",
       " 'VITIS_PROHIBITA___EXTRAIT_10',\n",
       " 'VITIS_PROHIBITA___EXTRAIT_4',\n",
       " 'Cepages_resistants']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('./datas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.shots.json\"\n",
      "\"/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.landmarks.txt\"\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "\n",
    "title = \"Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud\"\n",
    "path = pathlib.Path().parent.absolute()\n",
    "dir_path = \"\\\"\" + str(path) +\"/datas/\" + title + \"/\"\n",
    "shots_extension = \".shots.json\"\n",
    "track_extension = \".track.txt\"\n",
    "video_track_extension = \".track.mp4\"\n",
    "landmark_extension = \".landmarks.txt\"\n",
    "embedding_extension = \".embedding.txt\"\n",
    "labels_extension = \".labels.txt\"\n",
    "video_final_extension = \".final.mp4\"\n",
    "\n",
    "video_path = \"\\\"../videos/\" + title + \"/\" + title + \".mp4\\\"\"\n",
    "\n",
    "shots_path = dir_path + title + shots_extension + \"\\\"\"\n",
    "track_path = dir_path + title + track_extension + \"\\\"\"\n",
    "video_track_path = dir_path + title + video_track_extension + \"\\\"\"\n",
    "landmarks_path = dir_path + title + landmark_extension + \"\\\"\"\n",
    "embedding_path = dir_path + title + embedding_extension + \"\\\"\"\n",
    "labels_path = dir_path + title + labels_extension + \"\\\"\"\n",
    "video_final_path = dir_path + title + video_final_extension + \"\\\"\"\n",
    "\n",
    "print(shots_path)\n",
    "print(landmarks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "print(pathlib.Path().parent.absolute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shot segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit ici de vectoriser les prises de vues. En comparant avec la diarization, on peut repérer la présence d'un narrateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video structure\r\n",
      "\r\n",
      "The standard pipeline for is the following:\r\n",
      "\r\n",
      "    shot boundary detection ==> shot threading ==> segmentation into scenes\r\n",
      "\r\n",
      "Usage:\r\n",
      "  pyannote-structure.py shot [options] <video> <output.json>\r\n",
      "  pyannote-structure.py thread [options] <video> <shot.json> <output.json>\r\n",
      "  pyannote-structure.py scene [options] <video> <thread.json> <output.json>\r\n",
      "  pyannote-structure.py (-h | --help)\r\n",
      "  pyannote-structure.py --version\r\n",
      "\r\n",
      "Options:\r\n",
      "  --ffmpeg=<ffmpeg>      Specify which `ffmpeg` to use.\r\n",
      "  --height=<n_pixels>    Resize video frame to height <n_pixels> [default: 50].\r\n",
      "  --window=<n_seconds>   Apply median filtering on <n_seconds> window [default: 2.0].\r\n",
      "  --threshold=<value>    Set threshold to <value> [default: 1.0].\r\n",
      "  --min-match=<n_match>  Set minimum number of matches to <n_match> [default: 20].\r\n",
      "  --lookahead=<n_shots>  Look at up to <n_shots> following shots [default: 24].\r\n",
      "  -h --help              Show this screen.\r\n",
      "  --version              Show version.\r\n",
      "  --verbose              Show progress.\r\n"
     ]
    }
   ],
   "source": [
    "!pyannote-structure.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"../videos/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.mp4\"\n",
      "100%|██████████████████████████████████| 23.5k/23.5k [15:41<00:00, 25.0frames/s]\n"
     ]
    }
   ],
   "source": [
    "print(video_path)\n",
    "!pyannote-structure.py shot --verbose $video_path $shots_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detected shot boundaries can be visualized using `pyannote.core` notebook support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAABjCAYAAAAhK/2kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAK9klEQVR4nO3db6ykV10H8O/PvfUPEPeqrWZtiVtNiwLGIhtDoXFvxERMjSUGE0wwxPgSFY2JAV/xxkQNMRojJgQxTSASUkkkJBYIkib7proLS2qtGxtAKC0CL1RqDG3t8cXMLsPs7J3nmZ25c2bv55Ns7swz5znn9zzz/GbO/e2cudVaCwAAAAB9+bZtBwAAAADA1RRtAAAAADqkaAMAAADQIUUbAAAAgA4p2gAAAAB0aG9M45tvvrmdPn16Q6EAAAAAHD8XLlz4Wmvtlvnto4o2p0+fzvnz59cXFQAAAMAxV1X/vmi75VEAAAAAHVK0AQAAAOiQog0AAABAhxRtAAAAADqkaAMAAADQIUUbAAAAgA4p2gAAAAB0SNEGAAAAoEOKNgAAAAAdUrQBAAAA6JCiDQAAAECHFG0AAAAAOqRoAwAAANAhRRsAAACADinaAAAAAHRI0QYAAACgQ4o2AAAAAB1StAEAAADokKINAAAAQIcUbQAAAAA6pGgDAAAA0CFFGwAAAIAOKdoAAAAAdGhU0ebJJ5/MwcFB9vf3r3rs4OBg6f6X91u0/yLL+tzf37/S15jxl413WF9Dxllmto+qWtp26PlaNYZZ1xprtv1sm4ODg+zt7a0ztKvGvTz25ed7Uezzca/jedoVqxzrsn1mc2Fo21ViGXtt7+/vL7zehsR52GvFkP3HmO9r2f1rbRvy2LotOxdDz/M6xh57nlYZb96q8c++Nq379Wjd7zuMPx+L8uJGOaebzrNdcByOdx3zycP6GDKHvt6xx7RZlK97e3tXcnl+LrtOm5iXzRr7PG77+t72+Nu0zWMfkq9jrqWxc5vL+TbbfhPz3/n6wuXjXvQacD2/sxxmtu/Zn7NjDDnXh8VUrbXBAVXVlcbz+1XVVdsW7J/W2qC2Q/q8XPAY2ueQ/pb1NTT2oXGMOcZ1uta4Q7bP395EfLPjXu5/tsC17Ppbx/O0K1Y51jG5kBz+/I65nsfGsaj9onjGxLlozHVfx8uux2vFsMnXnaGWnYuh1846xh57nlYZb139H/Z+dL0xr/t9h/W89two53TTebYLjsPxruN97rA+xs7xVxl7zDx/0evwvFXnLuuI9Xr22fRca922Pf42bfPY152vY+c2i+Z0i+K53vnv/LFc6/fFIXPMVS2aA64y3nSfC621M/OPWR4FAAAA0KGVP2kDAAAAwFos/KTNyl9GYnnU6iyPGhfn5f4tj1rM8ijLo9bJ8ijLo44Dy6O+yfKo43G8lkdZHrUt2x5/myyPsjxqheVRC40q2pw6dSp33nlnLl68eNVjZ8+eXbr/yZMnv+XnMsv6nO1nzPjLxjusryHjLDOmj7Nnzy4835uK4VrnaLb9/Hk/d+7ceoNbMu5dd911Vbv5uNfxPO2KVY512T5DcmFRX2NjGfpaMNv+6aefPjSGZWMtarvu62W+v2X3l8VwlNfz0GvjWsY+p4f1vennalFfq8Z/8uTJK69N6349Our3neNg7Pk4iteNbbmRj22o43C865hPHtbHmHnDqmOPabPo/eTcuXO55557kuRbjmPTc4B17zP2fWrb1/e2x9+mbR77kHwdcy2NnducOHHiSr4d1v5657/z9YXLxz3/++KQOeaqFr32zPc/5FyfPXs2Dz300MLHRi2POnPmTDt//vzg9gAAAAAcrqp8ETEAAADArlC0AQAAAOiQog0AAABAhxRtAAAAADqkaAMAAADQIUUbAAAAgA4p2gAAAAB0SNEGAAAAoEOKNgAAAAAdUrQBAAAA6JCiDQAAAECHFG0AAAAAOqRoAwAAANAhRRsAAACADinaAAAAAHRI0QYAAACgQ4o2AAAAAB1StAEAAADokKINAAAAQIcUbQAAAAA6pGgDAAAA0CFFGwAAAIAOKdoAAAAAdKhaa8MbV309yaXNhQOs2c1JvrbtIIDB5CzsFjkLu0XO0rMfaq3dMr9xb2Qnl1prZ9YUELBhVXVezsLukLOwW+Qs7BY5yy6yPAoAAACgQ4o2AAAAAB0aW7R590aiADZFzsJukbOwW+Qs7BY5y84Z9UXEAAAAABwNy6MAAAAAOqRoAwAAANChQUWbqnpdVV2qqser6m2bDgpYrqpeXFWfrKrHqurRqnrrdPv3VtXHq+rfpj+/Z2aft0/z+FJV/dz2oofjq6pOVNWnq+oj0/tyFjpVVftV9UBV/ev0/fZuOQv9qqrfmc6L/7mq/qaqvlPOsuuWFm2q6kSSv0jy80lemuRXquqlmw4MWOq5JL/bWvuxJK9K8pZpbr4tySdaa3ck+cT0fqaPvTHJy5K8Lsm7pvkNHK23Jnls5r6chX79WZIHW2s/muQnMsldOQsdqqpbk/xWkjOttZcnOZFJTspZdtqQT9r8VJLHW2ufba09k+QDSe7bbFjAMq21p1prn5re/nomE8lbM8nP+6fN7k/y+unt+5J8oLX2jdba55I8nkl+A0ekqm5Lcm+S98xslrPQoar67iQ/neSvkqS19kxr7T8jZ6Fne0m+q6r2krwgyZORs+y4IUWbW5N8ceb+E9NtQCeq6nSSVyR5OMkPtNaeSiaFnSTfP20ml2H7/jTJ7yV5fmabnIU+/XCSryb56+mSxvdU1QsjZ6FLrbUvJXlnki8keSrJf7XWPhY5y44bUrSpBdv8nXDoRFW9KMnfJvnt1tp/H9Z0wTa5DEekqn4hyVdaaxeG7rJgm5yFo7OX5CeT/GVr7RVJ/ifTZRXXIGdhi6bfVXNfktuT/GCSF1bVmw7bZcE2OUt3hhRtnkjy4pn7t2XyMTNgy6rqpkwKNu9vrX1ouvk/qurU9PFTSb4y3S6XYbtek+QXq+rzmSw1/pmqel/kLPTqiSRPtNYent5/IJMijpyFPv1sks+11r7aWns2yYeSvDpylh03pGjzT0nuqKrbq+rbM/mypg9vNixgmaqqTNbZP9Za+5OZhz6c5M3T229O8ncz299YVd9RVbcnuSPJPx5VvHDctdbe3lq7rbV2OpP30n9orb0pcha61Fr7cpIvVtVLpptem+RfImehV19I8qqqesF0nvzaTL7zUc6y0/aWNWitPVdVv5Hko5l8A/d7W2uPbjwyYJnXJPnVJI9U1cXptt9P8odJPlhVv57Jm9cvJ0lr7dGq+mAmE87nkryltfZ/Rx82MEfOQr9+M8n7p/9x+dkkv5bJf3rKWehMa+3hqnogyacyycFPJ3l3khdFzrLDqjXL9gAAAAB6M2R5FAAAAABHTNEGAAAAoEOKNgAAAAAdUrQBAAAA6JCiDQAAAECHFG0AgO5V1fdV1cXpvy9X1Zemt5+uqndtOz4AgE3wJ78BgJ1SVe9I8nRr7Z3bjgUAYJN80gYA2FlVdVBVH5nefkdV3V9VH6uqz1fVL1XVH1fVI1X1YFXdNG33yqp6qKouVNVHq+rUdo8CAGAxRRsA4EbyI0nuTXJfkvcl+WRr7ceT/G+Se6eFmz9P8obW2iuTvDfJH2wrWACAw+xtOwAAgDX6+9bas1X1SJITSR6cbn8kyekkL0ny8iQfr6pM2zy1hTgBAJZStAEAbiTfSJLW2vNV9Wz75pf3PZ/JvKeSPNpau3tbAQIADGV5FABwnFxKcktV3Z0kVXVTVb1syzEBACykaAMAHButtWeSvCHJH1XVZ5JcTPLq7UYFALCYP/kNAAAA0CGftAEAAADokKINAAAAQIcUbQAAAAA6pGgDAAAA0CFFGwAAAIAOKdoAAAAAdEjRBgAAAKBD/w8cPfUO0wVsnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Timeline(uri=None, segments=[<Segment(0, 1.48)>, <Segment(1.48, 1.8)>, <Segment(1.8, 2.2)>, <Segment(2.2, 6.4)>, <Segment(6.4, 13.48)>, <Segment(13.48, 13.56)>, <Segment(13.56, 15.2)>, <Segment(15.2, 15.36)>, <Segment(15.36, 23.56)>, <Segment(23.56, 30.44)>, <Segment(30.44, 30.72)>, <Segment(30.72, 31.04)>, <Segment(31.04, 31.52)>, <Segment(31.52, 33.08)>, <Segment(33.08, 40.8)>, <Segment(40.8, 40.96)>, <Segment(40.96, 47.32)>, <Segment(47.32, 49.28)>, <Segment(49.28, 55.2)>, <Segment(55.2, 69.64)>, <Segment(69.64, 71.68)>, <Segment(71.68, 71.84)>, <Segment(71.84, 72)>, <Segment(72, 72.2)>, <Segment(72.2, 79.04)>, <Segment(79.04, 79.44)>, <Segment(79.44, 79.76)>, <Segment(79.76, 97.28)>, <Segment(97.28, 102.44)>, <Segment(102.44, 102.56)>, <Segment(102.56, 107.52)>, <Segment(107.52, 110.08)>, <Segment(110.08, 110.16)>, <Segment(110.16, 112.68)>, <Segment(112.68, 112.92)>, <Segment(112.92, 113.04)>, <Segment(113.04, 113.32)>, <Segment(113.32, 133)>, <Segment(133, 133.12)>, <Segment(133.12, 135.36)>, <Segment(135.36, 135.56)>, <Segment(135.56, 135.84)>, <Segment(135.84, 135.96)>, <Segment(135.96, 136.6)>, <Segment(136.6, 140.76)>, <Segment(140.76, 141.52)>, <Segment(141.52, 144.88)>, <Segment(144.88, 194.56)>, <Segment(194.56, 204.8)>, <Segment(204.8, 211.84)>, <Segment(211.84, 213.96)>, <Segment(213.96, 214.28)>, <Segment(214.28, 221.32)>, <Segment(221.32, 223.48)>, <Segment(223.48, 234.04)>, <Segment(234.04, 234.2)>, <Segment(234.2, 243.96)>, <Segment(243.96, 244.04)>, <Segment(244.04, 244.84)>, <Segment(244.84, 245.12)>, <Segment(245.12, 245.2)>, <Segment(245.2, 248.68)>, <Segment(248.68, 249.8)>, <Segment(249.8, 256.84)>, <Segment(256.84, 257.04)>, <Segment(257.04, 257.96)>, <Segment(257.96, 262.4)>, <Segment(262.4, 263.68)>, <Segment(263.68, 279.04)>, <Segment(279.04, 282.48)>, <Segment(282.48, 286.56)>, <Segment(286.56, 290.04)>, <Segment(290.04, 296)>, <Segment(296, 319.44)>, <Segment(319.44, 319.96)>, <Segment(319.96, 325.4)>, <Segment(325.4, 332.76)>, <Segment(332.76, 332.84)>, <Segment(332.84, 345.08)>, <Segment(345.08, 345.84)>, <Segment(345.84, 348.2)>, <Segment(348.2, 352.76)>, <Segment(352.76, 368.72)>, <Segment(368.72, 372.8)>, <Segment(372.8, 372.92)>, <Segment(372.92, 384.8)>, <Segment(384.8, 384.88)>, <Segment(384.88, 385.44)>, <Segment(385.44, 387)>, <Segment(387, 387.2)>, <Segment(387.2, 390.68)>, <Segment(390.68, 404.48)>, <Segment(404.48, 418.4)>, <Segment(418.4, 475.84)>, <Segment(475.84, 486.4)>, <Segment(486.4, 486.56)>, <Segment(486.56, 489.88)>, <Segment(489.88, 532.72)>, <Segment(532.72, 537.04)>, <Segment(537.04, 587.56)>, <Segment(587.56, 635.36)>, <Segment(635.36, 636.2)>, <Segment(636.2, 636.44)>, <Segment(636.44, 649.6)>, <Segment(649.6, 650.24)>, <Segment(650.24, 650.92)>, <Segment(650.92, 655.96)>, <Segment(655.96, 657.36)>, <Segment(657.36, 675.84)>, <Segment(675.84, 683.36)>, <Segment(683.36, 686.12)>, <Segment(686.12, 689.44)>, <Segment(689.44, 689.84)>, <Segment(689.84, 690)>, <Segment(690, 690.12)>, <Segment(690.12, 691.32)>, <Segment(691.32, 691.84)>, <Segment(691.84, 692.32)>, <Segment(692.32, 693.08)>, <Segment(693.08, 694.92)>, <Segment(694.92, 711.68)>, <Segment(711.68, 722.68)>, <Segment(722.68, 732.2)>, <Segment(732.2, 732.52)>, <Segment(732.52, 742.6)>, <Segment(742.6, 769.2)>, <Segment(769.2, 822.4)>, <Segment(822.4, 849)>, <Segment(849, 849.16)>, <Segment(849.16, 849.24)>, <Segment(849.24, 854.92)>, <Segment(854.92, 856.2)>, <Segment(856.2, 856.32)>, <Segment(856.32, 865.28)>, <Segment(865.28, 865.48)>, <Segment(865.48, 869.28)>, <Segment(869.28, 875.52)>, <Segment(875.52, 875.68)>, <Segment(875.68, 875.84)>, <Segment(875.84, 876.48)>, <Segment(876.48, 878.72)>, <Segment(878.72, 879.68)>, <Segment(879.68, 883.84)>, <Segment(883.84, 890.4)>, <Segment(890.4, 897.4)>, <Segment(897.4, 897.88)>, <Segment(897.88, 899)>, <Segment(899, 899.16)>, <Segment(899.16, 901.12)>, <Segment(901.12, 901.84)>, <Segment(901.84, 901.96)>, <Segment(901.96, 902.36)>, <Segment(902.36, 906.24)>, <Segment(906.24, 906.52)>, <Segment(906.52, 909.56)>, <Segment(909.56, 925.56)>, <Segment(925.56, 926.72)>, <Segment(926.72, 927.76)>, <Segment(927.76, 929.76)>, <Segment(929.76, 930.88)>, <Segment(930.88, 932.68)>, <Segment(932.68, 932.8)>, <Segment(932.8, 941)>, <Segment(941, 941.08)>, <Segment(941.08, 941.24)>])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyannote.core.json import load_from\n",
    "shots = load_from(\"/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.shots.json\")\n",
    "shots\n",
    "\n",
    "#cv2.imwrite(dir_path+title+'.shots.png', shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face processing\n",
    "\n",
    "Dans cette partie, on fait la détection de visage et le suivi sur la vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detection and tracking\r\n",
      "\r\n",
      "The standard pipeline is the following\r\n",
      "\r\n",
      "      face tracking => feature extraction => face clustering\r\n",
      "\r\n",
      "Usage:\r\n",
      "  pyannote-face track [options] <video> <shot.json> <tracking>\r\n",
      "  pyannote-face extract [options] <video> <tracking> <landmark_model> <embedding_model> <landmarks> <embeddings>\r\n",
      "  pyannote-face demo [options] <video> <tracking> <output>\r\n",
      "  pyannote-face (-h | --help)\r\n",
      "  pyannote-face --version\r\n",
      "\r\n",
      "General options:\r\n",
      "\r\n",
      "  --ffmpeg=<ffmpeg>         Specify which `ffmpeg` to use.\r\n",
      "  -h --help                 Show this screen.\r\n",
      "  --version                 Show version.\r\n",
      "  --verbose                 Show processing progress.\r\n",
      "\r\n",
      "Face tracking options (track):\r\n",
      "\r\n",
      "  <video>                   Path to video file.\r\n",
      "  <shot.json>               Path to shot segmentation result file.\r\n",
      "  <tracking>                Path to tracking result file.\r\n",
      "\r\n",
      "  --min-size=<ratio>        Approximate size (in video height ratio) of the\r\n",
      "                            smallest face that should be detected. Default is\r\n",
      "                            to try and detect any object [default: 0.0].\r\n",
      "  --every=<seconds>         Only apply detection every <seconds> seconds.\r\n",
      "                            Default is to process every frame [default: 0.0].\r\n",
      "  --min-overlap=<ratio>     Associates face with tracker if overlap is greater\r\n",
      "                            than <ratio> [default: 0.5].\r\n",
      "  --min-confidence=<float>  Reset trackers with confidence lower than <float>\r\n",
      "                            [default: 10.].\r\n",
      "  --max-gap=<float>         Bridge gaps with duration shorter than <float>\r\n",
      "                            [default: 1.].\r\n",
      "\r\n",
      "Feature extraction options (features):\r\n",
      "\r\n",
      "  <video>                   Path to video file.\r\n",
      "  <tracking>                Path to tracking result file.\r\n",
      "  <landmark_model>          Path to dlib facial landmark detection model.\r\n",
      "  <embedding_model>         Path to dlib feature extraction model.\r\n",
      "  <landmarks>               Path to facial landmarks detection result file.\r\n",
      "  <embeddings>              Path to feature extraction result file.\r\n",
      "\r\n",
      "Visualization options (demo):\r\n",
      "\r\n",
      "  <video>                   Path to video file.\r\n",
      "  <tracking>                Path to tracking result file.\r\n",
      "  <output>                  Path to demo video file.\r\n",
      "\r\n",
      "  --height=<pixels>         Height of demo video file [default: 400].\r\n",
      "  --from=<sec>              Encode demo from <sec> seconds [default: 0].\r\n",
      "  --until=<sec>             Encode demo until <sec> seconds.\r\n",
      "  --shift=<sec>             Shift result files by <sec> seconds [default: 0].\r\n",
      "  --landmark=<path>         Path to facial landmarks detection result file.\r\n",
      "  --label=<path>            Path to track identification result file.\r\n"
     ]
    }
   ],
   "source": [
    "!pyannote-face.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 23.5k/23.5k [10:04<00:00, 38.9frames/s]\n"
     ]
    }
   ],
   "source": [
    "!pyannote-face.py track --verbose --every=0.5 $video_path $shots_path $track_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face tracks can be visualized using `demo` mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio-ffmpeg in /home/yoanwm/anaconda3/envs/pyannote/lib/python3.6/site-packages (0.4.7)\n",
      "[MoviePy] >>>> Building video /home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.track.mp4\n",
      "[MoviePy] Writing audio in Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.trackTEMP_MPY_wvf_snd.mp3\n",
      "100%|███████████████████████████████████| 20755/20755 [00:11<00:00, 1768.20it/s]\n",
      "[MoviePy] Done.\n",
      "[MoviePy] Writing video /home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.track.mp4\n",
      "100%|███████████████████████████████████▉| 23531/23532 [01:49<00:00, 215.12it/s]\n",
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: /home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.track.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pyannote-face.py demo $video_path $track_path $video_track_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\"/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.track.mp4\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2815f6c12ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_track_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'''<video alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\"/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.track.mp4\"'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "video = io.open(video_track_path, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial landmarks and face embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 23.5k/23.5k [1:05:01<00:00, 6.03frames/s]\n"
     ]
    }
   ],
   "source": [
    "!pyannote-face.py extract --verbose $video_path \\\n",
    "                                    $track_path \\\n",
    "                                    ./dlib-models/shape_predictor_68_face_landmarks.dat \\\n",
    "                                    ./dlib-models/dlib_face_recognition_resnet_model_v1.dat \\\n",
    "                                    $landmarks_path \\\n",
    "                                    $embedding_path\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once embeddings are extracted, let's apply face track hierarchical agglomerative clustering.  \n",
    "The distance between two clusters is defined as the average euclidean distance between all embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.video.face.clustering import FaceClustering\n",
    "clustering = FaceClustering(threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.embedding.txt\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Timeline(uri=None, segments=[<Segment(2.2, 6.36)>, <Segment(6.4, 13.44)>, <Segment(13.56, 15.16)>, <Segment(15.36, 23.52)>, <Segment(23.56, 30.4)>, <Segment(40.96, 45.68)>, <Segment(45.48, 47.28)>, <Segment(47.32, 49.24)>, <Segment(49.28, 55.16)>, <Segment(55.2, 69.6)>, <Segment(69.64, 71.64)>, <Segment(72, 72.16)>, <Segment(79.76, 97.24)>, <Segment(123.16, 132.96)>, <Segment(135.84, 135.92)>, <Segment(136.12, 136.56)>, <Segment(136.6, 140.72)>, <Segment(140.76, 141.48)>, <Segment(141.52, 144.84)>, <Segment(149.24, 194.52)>, <Segment(194.56, 196)>, <Segment(199.4, 204.76)>, <Segment(204.8, 211.8)>, <Segment(211.84, 213.92)>, <Segment(213.96, 214.24)>, <Segment(214.28, 221.28)>, <Segment(221.32, 223.44)>, <Segment(223.48, 234)>, <Segment(234.2, 243.92)>, <Segment(244.04, 244.8)>, <Segment(245.2, 248.64)>, <Segment(248.68, 249.76)>, <Segment(249.8, 256.8)>, <Segment(257.04, 257.92)>, <Segment(257.96, 262.36)>, <Segment(262.4, 263.64)>, <Segment(263.68, 279)>, <Segment(279.04, 282.44)>, <Segment(282.48, 286.52)>, <Segment(286.56, 290)>, <Segment(290.04, 295.96)>, <Segment(296, 319.4)>, <Segment(319.44, 319.92)>, <Segment(319.96, 325.36)>, <Segment(325.4, 332.72)>, <Segment(332.84, 345.04)>, <Segment(348.2, 352.72)>, <Segment(352.76, 368.68)>, <Segment(368.72, 372.76)>, <Segment(372.92, 384.76)>, <Segment(384.88, 385.4)>, <Segment(385.44, 386.96)>, <Segment(387.2, 390.64)>, <Segment(390.68, 404.44)>, <Segment(404.48, 410.24)>, <Segment(410.52, 418.36)>, <Segment(418.4, 475.8)>, <Segment(475.84, 486.36)>, <Segment(486.56, 489.84)>, <Segment(489.88, 531.76)>, <Segment(531.68, 532.68)>, <Segment(532.72, 537)>, <Segment(537.04, 587.52)>, <Segment(587.56, 608.96)>, <Segment(608.92, 635.32)>, <Segment(635.36, 636.16)>, <Segment(636.44, 649.56)>, <Segment(657.36, 675.8)>, <Segment(675.84, 683.32)>, <Segment(683.36, 686.08)>, <Segment(686.12, 689.4)>, <Segment(690.12, 691.28)>, <Segment(691.32, 691.8)>, <Segment(691.84, 692.28)>, <Segment(692.32, 693.04)>, <Segment(693.08, 694.88)>, <Segment(694.92, 711.64)>, <Segment(711.68, 722.64)>, <Segment(722.68, 732.16)>, <Segment(732.2, 732.48)>, <Segment(732.52, 742.56)>, <Segment(742.6, 769.16)>, <Segment(769.2, 783.56)>, <Segment(783.44, 822.36)>, <Segment(822.4, 847.48)>, <Segment(846.84, 848.92)>])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding_path)\n",
    "face_tracks, embeddings = clustering.model.preprocess(\"/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.embedding.txt\")\n",
    "face_tracks.get_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = clustering(face_tracks, features=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyannote.core import notebook, Segment\n",
    "notebook.reset()\n",
    "#notebook.crop = Segment(0, 65)\n",
    "mapping = {}\n",
    "result = result.rename_labels(mapping=mapping)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_path)\n",
    "with open(\"/home/yoanwm/Documents/TER/TER-Evaluation-Transcription-automatique-d-audio/faceDetection/datas/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud/Laccave_seminaire_Ideoypes&Vins_de_demain_Eric_Giraud-Heraud.labels.txt\"\n",
    "\n",
    ", 'w') as fp:\n",
    "    for _, track_id, cluster in result.itertracks(yield_label=True):\n",
    "        fp.write(f'{track_id} {cluster}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyannote-face.py demo $video_path \\\n",
    "                       $track_path \\\n",
    "                       --label=$labels_path \\\n",
    "                       $video_final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "video = io.open(video_final_path, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(video_final_path,embed = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
